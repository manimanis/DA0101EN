<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Module 4 - Model Development</title>
  <link rel="stylesheet" href="css/bootstrap.min.css">
  <link rel="stylesheet" href="css/styles/default.css">
  <link rel="stylesheet" href="css/styles/androidstudio.css">
  <link rel="stylesheet" href="css/style.css">
</head>

<body>
  <main class="container my-4">
    <h1>Data Analysis with Python</h1>
    <h2>Module 4 - Model Development</h2>

    <section>
      <h3>Syllabus</h3>
      <ul>
        <li>Simple and Multiple Linear Regression</li>
        <li>Model Evaluation using Visualization</li>
        <li>Polynomial Regression and Pipelines</li>
        <li>R-squared and MSE for In-Sample Evaluation</li>
        <li>Prediction and Decision Making</li>
      </ul>
    </section>

    <section>
      <h3>Introduction</h3>

      <p>In this module you will learn about:</p>
      <ul>
        <li>Simple and Multiple Linear Regression</li>
        <li>Model Evaluation using Visualization</li>
        <li>Polynomial Regression and Pipelines</li>
        <li>R-squared and MSE for In-Sample Evaluation</li>
        <li>Prediction and Decision Making</li>
      </ul>

      <p><strong>This tools will be used to determine a fair value for a used car?</strong></p>

      <h4>Model</h4>

      <p>A model or estimator can be thought of as a mathematical equation used to predict a
        value given one or more other values.
        Relating one or more independent variables or features to dependent variables.</p>

      <p>For example, you input a car model’s highway miles per gallon (MPG) as the independent
        variable or feature, the output of the model or dependent variable is the price.</p>

      <figure>
        <img src="images/img40.png" alt="Model with one independent variable">
        <figcaption>Model with one independent variable</figcaption>
      </figure>

      <p>Usually the more relevant data you have the more accurate your model is. For example, you
        input multiple independent variables or features to your model. Therefore, your model may
        predict a more accurate price for the car.</p>

      <figure>
        <img src="images/img41.png" alt="Model with many independent variables">
        <figcaption>Model with many independent variables</figcaption>
      </figure>

      <p>To understand why more data is important, consider the following situation:</p>

      <ul>
        <li>You have two almost identical cars</li>
        <li>Pink cars sell for significantly less</li>
      </ul>

      <p>You want to use your model to determine the price of two cars, one pink, one red.
        If your model's independent variables or features do not include color, your model will
        predict the same price for cars that may sell for much less.</p>

      <figure>
        <img src="images/img42.png"
             alt="The importance of independent variables to predict the dependent variable">
        <figcaption>The importance of independent variables to predict the dependent variable
        </figcaption>
      </figure>

      <p>In addition to getting more data, you can try different types of models.</p>
      <ul>
        <li>Simple Linear Regression</li>
        <li>Multiple Linear Regression</li>
        <li>Polynomial Regression</li>
      </ul>
    </section>

    <section>
      <h3>Linear and Multiple Linear Regression</h3>

      <p>Simple Linear Regression (or SLR) refers to one independent variable to make a prediction.
      </p>
      <p>Multiple Linear Regression (or MLR) refers to multiple independent variables to make a
        prediction.
      </p>

      <figure>
        <img src="images/img43.png"
             alt="The difference between Linear Regression and Multiple Linear Regression">
        <figcaption>The difference between Linear Regression and Multiple Linear Regression
        </figcaption>
      </figure>

      <p>Simple Linear Regression is A method that is used to understand the relationship between
        two variables:</p>
      <ul>
        <li>The predictor (independent) variable x,</li>
        <li>and the target (dependent) variable y.</li>
      </ul>

      <p class="display-3 text-center">y = b<sub>0</sub> + b<sub>1</sub>.x</p>

      <ul>
        <li>The parameter b<sub>0</sub> is the intercept</li>
        <li>The parameter b<sub>1</sub> is the slope</li>
      </ul>

      <p>When we fit or train the model, we will come up with these parameters.</p>

      <p><strong>Let’s clarify the prediction step.</strong> It’s hard to figure out how much a car
        costs, but if we assume that there is a linear relationship between the Highway Miles per
        Gallon and the price of the car, we can use it to formulate
        a model to determine the price of the car.</p>

      <figure>
        <img src="images/img44.png"
             alt="Using the SLR to predict the price of the car in function of Highway MPG">
        <figcaption>Using the SLR to predict the price of the car in function of Highway MPG
        </figcaption>
      </figure>

      <p>If the Highway Miles per Gallon is 20, we can input his value into the model to obtain a
        prediction of $22,000.</p>


      <p>In many cases, many factors influence how much people pay for a car, for example, make or
        how old the car is.
        In this model, this uncertainty is taken into account by assuming a small random value is
        added to the point
        on the line; this is called noise.</p>

      <figure>
        <img src="images/img45.png" alt="The noise in the values of the cars">
        <figcaption>The noise in the values of the cars</figcaption>
      </figure>

      <pre><code class="python">from sklearn.linear_model import LinearRegression
# define the predictor variable X and the target variable y
X = df[['highway-mpg']]
y = df['price']
# use linear regression
lr = LinearRegression()
lr.fit(X, y)</code></pre>

      <p>When we fit the model it calculates :</p>

      <ul>
        <li>the intercept b0</li>
        <li>the slope b1</li>
      </ul>

      <pre><code class="python">print("yhat = ", lr.intercept_, "+", lr.coef_[0], ".x")</code></pre>

      <figure>
        <img src="images/img46.png" alt="The intercept and the slope of the linear equation">
        <figcaption>The intercept and the slope of the linear equation</figcaption>
      </figure>

      <pre><code class="python">x = pd.DataFrame(data={"highway-mpg": [20]})
yhat = lr.predict(x)
print(f"price({x.iloc[0][0]} highway-mpg) = {yhat[0]}$")</code></pre>

      <figure>
        <img src="images/img47.png" alt="Using the model to predict new prices">
        <figcaption>Using the model to predict prices</figcaption>
      </figure>


      <p>Multiple Linear Regression is used to explain the relationship between
        - One continuous target (Y) variable, and - Two or more predictor (X) variables.</p>

      <p class="text-center display-5">y = b0 + b1.x1 + b2.x2 + b3.x3 + ... + bn.xn</p>

      <p>If we have for example 4 predictor variables, then:</p>
      <ul>
        <li>B0: intercept</li>
        <li>B1: the coefficient or parameter of X1</li>
        <li>B2: the coefficient of parameter X2: and so on</li>
      </ul>

      <p>If there are only two variables then we can visualize the values in the space.</p>

      <figure>
        <img src="images/img48.png" alt="MLR Representation with two preditors variables">
        <figcaption>MLR Representation with two preditors variables</figcaption>
      </figure>

      <pre><code class="python"># define the predictor variables
X = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]
y = df['price']
# Use linear regression
mlr = LinearRegression()
mlr.fit(X, y)</code></pre>

      <p>We can find the different coefficients of the parameters.</p>

      <pre><code class="python">print('b0 = ', mlr.intercept_)
print('b1 = ', mlr.coef_[0])
print('b2 = ', mlr.coef_[1])
print('b3 = ', mlr.coef_[2])
print('b4 = ', mlr.coef_[3])</code></pre>

      <figure>
        <img src="images/img49.png" alt="The coefficients and the intercept of the MLR">
        <figcaption>The coefficients and the intercept of the MLR</figcaption>
      </figure>
    </section>

    <section>
      <h3>Model Evaluation using Visualization</h3>

      <p>Regression plots are a good estimate of:</p>
      <ul>
        <li>The relationship between two variables,</li>
        <li>The strength of the correlation, and</li>
        <li>The direction of the relationship (positive or negative).
        </li>
      </ul>

      <p>There are several ways to plot a regression plot; a simple way is to use "regplot" from
        the seaborn library.</p>

      <pre><code class="python">sns.regplot(x="highway-mpg", y="price", data=df)
plt.ylim(0)</code></pre>

      <figure>
        <img src="images/img50.png" alt="Regression plot">
        <figcaption>Regression plot</figcaption>
      </figure>

      <p>The residual plot represents the error between the actual values and the predicted values.
        We obtain this plot by subtracting the predicted value and the actual target value.</p>

      <pre><code class="python">sns.residplot(x="highway-mpg", y="price", data=df)</code></pre>

      <figure>
        <img src="images/img51.png" alt="Residual plot">
        <figcaption>Residual plot</figcaption>
      </figure>

      <p>We see in this case the Residuals have a curvature.</p>

      <p>Looking at the plot gives us some insight into our data.
        We expect to see the results to have zero mean.</p>
      <p>This type of residual plot suggests a linear plot is appropriate.</p>

      <figure>
        <img src="images/img52.png" alt="Regression plot">
        <img src="images/img53.png" alt="Residual plot">
        <figcaption>Regression plot, Residual plot</figcaption>
      </figure>

      <p>In this residual plot there is curvature, the values of the error change with x.</p>

      <figure>
        <img src="images/img54.png" alt="Residual plot">
        <img src="images/img55.png" alt="Residual plot">
        <figcaption>Residual plot of non linear functions</figcaption>
      </figure>

      <p>This plot suggests a non-linear function.</p>

      <p>A distribution plot counts the predicted value versus the actual value.
        These plots are extremely useful for visualizing models with more than one independent
        variable or feature.</p>

      <p>Let's look at a simplified example:</p>
      <figure>
        <img src="images/img56.png" alt="Line plot">
        <figcaption>Line plot of the prices</figcaption>
      </figure>

      <ul>
        <li>We examine the vertical axis for <strong>Price category 1</strong>.</li>
        <li>We then count and plot the number of predicted points that are between [0, 5[.</li>
        <li>We then count and plot the number of predicted points that are between [5, 10[</li>
        <li>We then count and plot the number of predicted points that are between [10, 15[</li>
      </ul>

      <ul>
        <li>We examine the vertical axis for <strong>Price category 2</strong>.</li>
        <li>We then count and plot the number of predicted points that are between [5, 10[</li>
      </ul>

      <figure>
        <img src="images/img57.png" alt="Distribution plot">
        <figcaption>Distribution plot of the prices</figcaption>
      </figure>

      <p> The values of the targets and predicted values are continuous.
        A histogram is for discrete values.
        Therefore pandas will convert them to a distribution.
        The vertical axis is scaled to make the area under the distribution equal to one.</p>

      <p>This is an example of using a distribution plot.</p>
      <pre><code class="python">Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]
Yhat = mlr.predict(Z)
plt.figure(figsize=(width, height))

ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values" , ax=ax1)

plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price (in dollars)')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()</code></pre>

      <ul>
        <li>The dependent variable or feature is price.</li>
        <li>The fitted values that result from the model are in blue.</li>
        <li>The actual values are in red.</li>
      </ul>

      <figure>
        <img src="images/img58.png" alt="Distribution plot">
        <figcaption>Distribution plot of the prices</figcaption>
      </figure>

      <p>In this example, we use multiple features or independent variables. Predicted values are
        much closer to the target values.</p>

      <pre><code class="python">X = df[['highway-mpg']]
Yhat = lr.predict(X)
plt.figure(figsize=(width, height))

ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values" , ax=ax1)

plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price (in dollars)')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()</code></pre>

      <figure>
        <img src="images/img59.png" alt="Distribution plot">
        <figcaption>Distribution plot of the prices</figcaption>
      </figure>
      <p>We see the predicted values for prices in the range from $40 000 to $50 000 are inaccurate.
        The prices in the region form $10 000 to $20 000 are much closer to the target value.
      </p>
    </section>

    <section>
      <h3>Polynomial Regression and Pipelines</h3>

      <p>Polynomial regression is a special case of the general linear regression.
        This method is beneficial for describing curvilinear relationships.</p>

      <p><strong>What is a curvilinear relationship?</strong>
        It’s what you get by squaring or setting higher-order terms of the predictor variables
        in the model, transforming the data.</p>

      <p>The model can be quadratic, which means that the predictor variable in the model is
        squared.</p>

      <p class="display-4 text-center">&Yacute; = b<sub>0</sub> + b<sub>1</sub>.x + b<sub>2</sub>.x²
      </p>

      <figure>
        <img src="images/img60.png" alt="Quadratic Model">
        <figcaption>Quadratic Model</figcaption>
      </figure>

      <p>The model can be cubic, which means that the predictor variable is cubed.
        This is a third order Polynomial regression.</p>

      <p class="display-4 text-center">&Yacute; = b<sub>0</sub> + b<sub>1</sub>.x + b<sub>2</sub>.x²
        + b<sub>3</sub>.x<sup>3</sup></p>

      <figure>
        <img src="images/img61.png" alt="Cubic Model">
        <figcaption>Cubic Model</figcaption>
      </figure>

      <p>There also exists higher order polynomial regressions, when a good fit hasn’t been
        achieved by second or third order.</p>

      <p class="display-4 text-center">&Ycirc; = b<sub>0</sub> + b<sub>1</sub>.x + b<sub>2</sub>.x²
        + b<sub>3</sub>.x<sup>3</sup> + ... + b<sub>n</sub>.x<sup>n</sup></p>

      <p>We use the "preprocessing" library in sci-kit-learn, to create a polynomial feature object.
      </p>

      <pre><code class="python">from sklearn.preprocessing import PolynomialFeatures
pr = PolynomialFeatures(degree=2)
xpoly = pr.fit_transform(df[['horsepower', 'curb-weight']])
xpoly</code></pre>

      <figure>
        <img src="images/img62.png" alt="Creating a polynomial feature object">
        <figcaption>Creating a polynomial feature object</figcaption>
      </figure>

      <p>Let's do a more intuitive example.
        Consider the features shown here.</p>

      <pre><code class="python">sdf = pd.DataFrame(data={'X1': [1,2], 'X2': [2,3]}) 
pr = PolynomialFeatures(degree=2)
xp = pr.fit_transform(sdf)
xp</code></pre>

      <figure>
        <img src="images/img63.png" alt="Creating a polynomial feature object">
        <figcaption>Creating a polynomial feature object</figcaption>
      </figure>

      <p>X1: 1, X2: 2, X1.X2: 2, X1²: 1, X2²: 4</p>

      <p> As the dimension of the data gets larger we may want to normalize multiple features in
        scikit-learn, instead, we can use the preprocessing module to simplify many tasks.</p>

      <p>For example, we can Standardize each feature simultaneously.
        We import “StandardScaler” We train the object, fit the scale object;
        then transform the data into a new dataframe on array “x_scale”.
        There are more normalization methods available in the preprocessing library, as well as
        other
        transformations.</p>

      <pre><code class="python">from sklearn.preprocessing import StandardScaler
scale = StandardScaler()
scale.fit(df[['horsepower', 'highway-mpg']])
x_scale = scale.transform(df[['horsepower', 'highway-mpg']])
x_scale</code></pre>

      <figure>
        <img src="images/img64.png" alt="Standard Scaler">
        <figcaption>Standard Scaler</figcaption>
      </figure>

      <h4>Pipelines</h4>
      <p>There are many steps to getting a prediction, for example,</p>
      <ul>
        <li>Normalization,</li>
        <li>Polynomial transform,</li>
        <li>and Linear regression.</li>
      </ul>
      <p>We can simplify our code by using a pipeline library.</p>

      <p>Pipelines sequentially perform a series of transformation.
        The last step carries out a prediction.</p>

      <figure>
        <img src="images/img65.png" alt="Using a pipeline">
        <figcaption>Using a pipeline</figcaption>
      </figure>

      <pre><code class="python">from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

input_ = [
    ("scale", StandardScaler()),
    ("Polynomial", PolynomialFeatures()),
    ("model", LinearRegression())
]

pipe = Pipeline(input_)

X = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]
y = df['price']

pipe.fit(X, y)

yhat = pipe.predict(X)</code></pre>

      <pre><code class="python">plt.plot(y)
plt.plot(yhat)</code></pre>

      <figure>
        <img src="images/img66.png" alt="Using a pipeline">
        <figcaption>Using a pipeline</figcaption>
      </figure>
    </section>

    <section>
      <h3>In sample evaluation</h3>

      <p>Now that we’ve seen how we can evaluate a model by using visualization, we want to
        numerically evaluate our models.</p>

      <p>Two important measures that we often use to determine the fit of a model are:</p>
      <ul>
        <li>Mean Square Error (MSE): we find the difference between the actual value y and the
          predicted value yhat then square it.</li>
        <li>and R-squared: R-squared is also called the coefficient of determination. It’s a measure
          to determine how close the data is to the fitted regression line. So how close is our
          actual data to
          our estimated model?</li>
      </ul>

      <figure>
        <img src="images/img67.png" alt="MSE formula">
        <img src="images/img68.png" alt="R² formula">
        <figcaption>MSE formula, R² formula</figcaption>
      </figure>

      <p>To find the MSE in Python, we can import the “mean_Squared_error()” from
        “scikit-learn.metrics”.
        The “mean_Squared_error()” function gets two inputs: the actual value of target variable
        and the predicted value of target variable.</p>

      <pre><code>from sklearn.metrics import mean_squared_error
mean_squared_error(y, yhat)</code></pre>

      <pre><code>9656613.4039896</code></pre>

      <p>We find the R-squared value in Python by using the score() method, in the linear regression
        object.</p>

      <pre><code class="python">lr = LinearRegression()
X = df[['highway-mpg']]
y = df['price']
lr.fit(X, y)
lr.score(X, y)</code></pre>

      <pre><code>0.4965911884339176</code></pre>

      <p>From the value that we get from this example, we can say that approximately 49.695% of the
        variation of price is explained by this simple linear model.</p>
    </section>
  </main>
  <script src="js/bootstrap.bundle.min.js"></script>
  <script src="js/highlight.pack.js"></script>
  <script>hljs.highlightAll();</script>
</body>

</html>